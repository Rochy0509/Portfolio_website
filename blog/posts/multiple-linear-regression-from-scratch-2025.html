<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  
  <title>Build Your First Machine Learning Model: Multiple Linear Regression From Scratch (Complete Guide 2025) | Kenneth Martinez Blog</title>
  <meta name="description" content="Learn multiple linear regression by building it from scratch using Python. This comprehensive guide shows you how to predict student performance with 98.8% accuracy without using scikit-learn. Perfect for beginners and self-taught ML enthusiasts.">
  <meta name="keywords" content="machine learning, multiple linear regression, python, from scratch, data science, tutorial, beginner, Kenneth Martinez, Robotics, Mechatronics Engineering">
  <meta name="author" content="Kenneth Martinez">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://kenneth-martinez.com/blog/posts/multiple-linear-regression-from-scratch-2025.html">
  <meta property="og:title" content="Build Your First Machine Learning Model: Multiple Linear Regression From Scratch (Complete Guide 2025)">
  <meta property="og:description" content="Learn multiple linear regression by building it from scratch using Python. This comprehensive guide shows you how to predict student performance with 98.8% accuracy without using scikit-learn. Perfect for beginners and self-taught ML enthusiasts.">
  <meta property="og:image" content="https://kenneth-martinez.com/blog/images/linear_fit.png">
  <meta property="article:author" content="Kenneth Martinez">
  <meta property="article:published_time" content="2025-01-12T00:00:00Z">
  <meta property="article:section" content="Machine Learning">

  <!-- Twitter -->
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:url" content="https://kenneth-martinez.com/blog/posts/multiple-linear-regression-from-scratch-2025.html">
  <meta property="twitter:title" content="Build Your First Machine Learning Model: Multiple Linear Regression From Scratch (Complete Guide 2025)">
  <meta property="twitter:description" content="Learn multiple linear regression by building it from scratch using Python. This comprehensive guide shows you how to predict student performance with 98.8% accuracy without using scikit-learn. Perfect for beginners and self-taught ML enthusiasts.">
  <meta property="twitter:image" content="https://kenneth-martinez.com/blog/images/linear_fit.png">

  <!-- Canonical URL -->
  <link rel="canonical" href="https://kenneth-martinez.com/blog/posts/multiple-linear-regression-from-scratch-2025.html">

  <!-- Favicons -->
  <link href="../../assets/img/favicon.png" rel="icon">
  <link href="../../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../../assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../../assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="../../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../../assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="../../assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../../assets/css/style.css" rel="stylesheet">

  <!-- Prism.js for Code Highlighting -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">

  <!-- Blog Post CSS -->
  <style>
    .blog-post-container {
      max-width: 800px;
      margin: 0 auto;
      padding: 20px;
      line-height: 1.8;
    }
    
    .blog-post-header {
      margin-bottom: 40px;
      text-align: center;
    }
    
    .back-to-blog {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      color: #0563bb;
      text-decoration: none;
      font-weight: 500;
      margin-bottom: 30px;
      transition: color 0.3s ease;
    }
    
    .back-to-blog:hover {
      color: #034a85;
    }
    
    .post-meta {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 20px;
      margin-bottom: 30px;
      font-size: 0.9rem;
      color: #728394;
      flex-wrap: wrap;
    }
    
    .post-category {
      background: #0563bb;
      color: white;
      padding: 6px 15px;
      border-radius: 25px;
      font-size: 0.8rem;
      font-weight: 500;
    }
    
    .post-title {
      font-size: 2.5rem;
      color: #45505b;
      margin-bottom: 20px;
      line-height: 1.3;
    }
    
    .featured-image {
      width: 100%;
      height: auto;
      max-width: 100%;
      border-radius: 15px;
      margin-bottom: 40px;
      box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
    }
    
    .post-content {
      font-size: 1.1rem;
      color: #45505b;
    }
    
    .post-content h2 {
      font-size: 1.8rem;
      color: #45505b;
      margin-top: 40px;
      margin-bottom: 20px;
      border-bottom: 2px solid #0563bb;
      padding-bottom: 10px;
    }
    
    .post-content h3 {
      font-size: 1.4rem;
      color: #45505b;
      margin-top: 30px;
      margin-bottom: 15px;
    }
    
    .post-content p {
      margin-bottom: 20px;
    }
    
    .post-content img {
      width: 100%;
      height: auto;
      border-radius: 10px;
      margin: 20px 0;
      box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
    }
    
    .post-content blockquote {
      background: #f8f9fa;
      border-left: 4px solid #0563bb;
      padding: 20px;
      margin: 30px 0;
      border-radius: 0 10px 10px 0;
      font-style: italic;
    }
    
    .post-content ul, .post-content ol {
      padding-left: 30px;
      margin-bottom: 20px;
    }
    
    .post-content li {
      margin-bottom: 8px;
    }
    
    .post-content code {
      background: #f1f3f4;
      padding: 2px 6px;
      border-radius: 4px;
      font-family: 'Courier New', monospace;
      font-size: 0.9em;
    }
    
    .post-content pre {
      background: #2d3748;
      color: #e2e8f0;
      padding: 20px;
      border-radius: 10px;
      overflow-x: auto;
      margin: 20px 0;
    }
    
    .post-content pre code {
      background: none;
      padding: 0;
      color: inherit;
    }
    
    .adsense-container {
      margin: 40px 0;
      text-align: center;
      min-height: 250px;
      background: #f8f9fa;
      border: 2px dashed #ddd;
      border-radius: 10px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: #999;
      font-style: italic;
    }
    
    .post-tags {
      margin-top: 40px;
      padding-top: 20px;
      border-top: 1px solid #eee;
    }
    
    .post-tags h4 {
      margin-bottom: 15px;
      color: #45505b;
    }
    
    .tag {
      display: inline-block;
      background: #f1f3f4;
      color: #45505b;
      padding: 5px 12px;
      border-radius: 20px;
      font-size: 0.9rem;
      margin-right: 10px;
      margin-bottom: 10px;
      text-decoration: none;
      transition: background 0.3s ease;
    }
    
    .tag:hover {
      background: #0563bb;
      color: white;
    }
    
    .author-bio {
      background: #f8f9fa;
      padding: 30px;
      border-radius: 15px;
      margin-top: 50px;
      text-align: center;
    }
    
    .author-bio img {
      width: 100px;
      height: 100px;
      border-radius: 50%;
      margin-bottom: 20px;
    }
    
    .social-share {
      margin-top: 40px;
      text-align: center;
    }
    
    .social-share h4 {
      margin-bottom: 20px;
      color: #45505b;
    }
    
    .share-buttons {
      display: flex;
      justify-content: center;
      gap: 15px;
      flex-wrap: wrap;
    }
    
    .share-btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 20px;
      border-radius: 25px;
      text-decoration: none;
      color: white;
      font-weight: 500;
      transition: transform 0.3s ease;
    }
    
    .share-btn:hover {
      transform: translateY(-2px);
      color: white;
    }
    
    .share-twitter { background: #1da1f2; }
    .share-linkedin { background: #0077b5; }
    .share-facebook { background: #1877f2; }
    
    .read-time-bar {
      background: #f8f9fa;
      padding: 15px;
      border-radius: 10px;
      text-align: center;
      margin-bottom: 30px;
      font-weight: 500;
      color: #45505b;
    }
    
    /* Responsive Design */
    @media (max-width: 768px) {
      .blog-post-container {
        padding: 15px;
      }
      
      .post-title {
        font-size: 2rem;
      }
      
      .post-meta {
        flex-direction: column;
        gap: 10px;
      }
      
      .share-buttons {
        flex-direction: column;
        align-items: center;
      }
      
      .share-btn {
        width: 200px;
        justify-content: center;
      }
    }
  </style>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2650NSHRTX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-2650NSHRTX');
  </script>

  <!-- JSON-LD Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Build Your First Machine Learning Model: Multiple Linear Regression From Scratch (Complete Guide 2025)",
    "description": "Learn multiple linear regression by building it from scratch using Python. This comprehensive guide shows you how to predict student performance with 98.8% accuracy without using scikit-learn. Perfect for beginners and self-taught ML enthusiasts.",
    "image": "https://kenneth-martinez.com/blog/images/linear_fit.png",
    "author": {
      "@type": "Person",
      "name": "Kenneth Martinez",
      "jobTitle": "Mechatronics Engineer",
      "affiliation": "Ontario Tech University"
    },
    "publisher": {
      "@type": "Person",
      "name": "Kenneth Martinez"
    },
    "datePublished": "2025-07-12T00:00:00Z",
    "dateModified": "2025-07-12T00:00:00Z",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://kenneth-martinez.com/blog/posts/multiple-linear-regression-from-scratch-2025.html"
    }
  }
  </script>
</head>

<body>
  <main class="blog-post-container">
    <a href="../index.html" class="back-to-blog">
      <i class="bi bi-arrow-left"></i>
      Back to Blog
    </a>
    
    <header class="blog-post-header">
      <div class="post-meta">
        <span class="post-category">Machine Learning</span>
        <span><i class="bi bi-calendar3"></i> July 12, 2025</span>
        <span><i class="bi bi-clock"></i> 12 min read</span>
        <span><i class="bi bi-person"></i> Kenneth Martinez</span>
      </div>
      
      <h1 class="post-title">Build Your First Machine Learning Model: Multiple Linear Regression From Scratch (Complete Guide 2025)</h1>
      
      <div class="read-time-bar">
        <strong>Read Time:</strong> 12 minutes | <strong>Difficulty:</strong> Beginner-Intermediate | <strong>What You Will Build:</strong> A complete ML model from scratch
      </div>
      
      <img src="../images/linear_fit.png" alt="Linear regression model fitting visualization showing the relationship between features and predictions" class="featured-image" loading="lazy">
    </header>

    <article class="post-content">
      
      <h2>Table of Contents</h2>
      <ol>
        <li><a href="#what-you-will-learn">What You Will Learn</a></li>
        <li><a href="#understanding-multiple-linear-regression">Understanding Multiple Linear Regression</a></li>
        <li><a href="#the-student-performance-dataset">The Student Performance Dataset</a></li>
        <li><a href="#building-your-model-step-by-step">Building Your Model Step by Step</a></li>
        <li><a href="#validating-your-results">Validating Your Results</a></li>
        <li><a href="#key-takeaways">Key Takeaways</a></li>
      </ol>

      <h2 id="what-you-will-learn">What You Will Learn</h2>
      
      <p>In this tutorial, you will discover how to build a multiple linear regression model completely from scratch. By the end, you will understand not just how to use machine learning, but how it actually works under the hood. This knowledge will set you apart from developers who only know how to import libraries.</p>

      <h3>Why Build From Scratch?</h3>
      <p>Building models from scratch provides three key advantages:</p>
      <ul>
        <li><strong>Deep Understanding:</strong> You learn the mathematics behind the algorithms</li>
        <li><strong>Better Debugging:</strong> When things go wrong, you know exactly where to look</li>
        <li><strong>Interview Readiness:</strong> Top companies often ask candidates to implement algorithms from scratch</li>
      </ul>
      
      <p>Let me guide you through building a model that predicts student performance with remarkable accuracy.</p>

      <h2 id="understanding-multiple-linear-regression">Understanding Multiple Linear Regression</h2>
      
      <p>Before diving into code, let us establish a solid foundation. Multiple linear regression finds the best-fitting line (or hyperplane) through multi-dimensional data. Think of it as drawing the most accurate trend line possible through a cloud of data points.</p>
      
      <p>The mathematical formula at the heart of our model is:</p>
      
      <blockquote>
        <p><strong>y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ</strong></p>
      </blockquote>
      
      <p>Where:</p>
      <ul>
        <li><strong>y</strong> represents what we want to predict (student performance)</li>
        <li><strong>β values</strong> are coefficients showing how much each factor matters</li>
        <li><strong>x values</strong> are our input features (study hours, previous scores, etc.)</li>
      </ul>

      <h3>The Normal Equation: Your Secret Weapon</h3>
      
      <p>Instead of using iterative methods like gradient descent, we will use the normal equation for an elegant, one-shot solution:</p>
      
      
      <blockquote>
        <p><strong>β = (X^T × X)^(-1) × X^T × y</strong></p>
      </blockquote>
      
      <p>This equation finds the optimal coefficients instantly. No loops. No iterations. Just pure mathematical precision.</p>

      <h2 id="the-student-performance-dataset">The Student Performance Dataset</h2>
      
      <p>Our journey begins with real data from Kaggle containing information about 1,000 students. Each row tells a story about a student's habits and their resulting academic performance.</p>
      
      <p>The dataset includes six columns:</p>
      <ul>
        <li><strong>Hours Studied:</strong> Daily study time (1-10 hours)</li>
        <li><strong>Previous Scores:</strong> Historical performance (0-100)</li>
        <li><strong>Extracurricular Activities:</strong> Yes/No participation</li>
        <li><strong>Sleep Hours:</strong> Nightly rest (4-10 hours)</li>
        <li><strong>Sample Question Papers Practiced:</strong> Exam preparation (0-10)</li>
        <li><strong>Performance Index:</strong> Our target variable (0-100)</li>
      </ul>
      
      <p>You can access the complete implementation and dataset here: <a href="#" target="_blank">Kaggle Notebook - Multi Linear Regression From Scratch</a></p>

      <h2 id="building-your-model-step-by-step">Building Your Model Step by Step</h2>

      <h3>Step 1: Data Exploration and Feature Selection</h3>
      
      <p>The first rule of machine learning is to understand your data before touching any algorithms. Let us start by examining which factors actually influence student performance.</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Load the dataset
data_df = pd.read_csv("Student_Performance.csv")

# Convert categorical data to numerical
data_df['Extracurricular Activities'] = data_df['Extracurricular Activities'].map({'Yes':1, 'No':0})

# Calculate correlations with our target variable
corr_m = data_df.corr()
print(corr_m['Performance Index'])</code></pre>

      <img src="../images/correlation_ML.png" alt="Correlation heatmap showing relationships between all variables in the student performance dataset" style="max-width: 500px; margin: 20px auto; display: block;">
      
      <p>The correlation analysis reveals fascinating insights:</p>
      <ul>
        <li><strong>Previous Scores (0.92):</strong> Nearly perfect correlation - past performance strongly predicts future success</li>
        <li><strong>Hours Studied (0.37):</strong> Moderate impact - effort matters, but it is not everything</li>
        <li><strong>Extracurricular Activities (0.02):</strong> Surprisingly minimal impact</li>
        <li><strong>Sleep Hours (0.05):</strong> Low correlation, but we will keep it to capture potential hidden patterns</li>
      </ul>
      
      <p>Based on this analysis, we select three features for our model: Previous Scores, Hours Studied, and Sleep Hours.</p>

      <h3>Step 2: Data Preprocessing - The Foundation of Success</h3>
      
      <p>Clean, well-prepared data is essential for any machine learning model. Our preprocessing function handles three critical tasks:</p>

<pre><code class="language-python">def data_processing(data_df, features_columns, target_column, train_size=0.8, random_state=42):
    # Select only the features we need
    desired_data = data_df[features_columns + [target_column]].copy()
    
    # Shuffle to ensure random distribution
    shuffle_df = desired_data.sample(frac=1, random_state=random_state).reset_index(drop=True)
    
    # Split into training and testing sets
    train_sz = int(train_size * len(desired_data))
    train_set = shuffle_df[:train_sz]
    test_set = shuffle_df[train_sz:]
    
    # Separate features and target
    X_train = train_set[features_columns]
    y_train = train_set[target_column]
    X_test = test_set[features_columns]
    y_test = test_set[target_column]
    
    # Standardization - bringing all features to the same scale
    train_mean = X_train.mean()
    train_std = X_train.std()
    X_train_std = (X_train - train_mean) / train_std
    X_test_std = (X_test - train_mean) / train_std
    
    return X_train_std, y_train, X_test_std, y_test</code></pre>

      <p><strong>Why Standardization Matters:</strong> Without standardization, features with larger scales (like Previous Scores: 0-100) would dominate features with smaller scales (like Hours Studied: 1-10). Standardization ensures every feature competes fairly.</p>

      <h3>Step 3: Implementing the Learning Algorithm</h3>
      
      <p>Now comes the exciting part - implementing the actual learning algorithm. This is where mathematical theory transforms into working code:</p>

<pre><code class="language-python">def train_and_eval_model(X_train, y_train, X_test, y_test, plot_residuals=True):
    # Add intercept column (for the β₀ term)
    n_train = X_train.shape[0]
    n_test = X_test.shape[0]
    X_train_intercept = np.column_stack([np.ones(n_train), X_train])
    X_test_intercept = np.column_stack([np.ones(n_test), X_test])
    
    # Apply the normal equation
    X_T = X_train_intercept.T
    try:
        # This single line finds the optimal coefficients
        beta = np.linalg.solve(X_T @ X_train_intercept, X_T @ y_train)
    except np.linalg.LinAlgError:
        # Fallback for singular matrices
        beta = np.linalg.lstsq(X_train_intercept, y_train, rcond=None)[0]
    
    # Generate predictions
    y_train_pred = X_train_intercept @ beta
    y_test_pred = X_test_intercept @ beta
    
    # Calculate performance metrics
    # ... (metrics calculation code continues)</code></pre>

      <p>The beauty of this approach lies in its elegance. While gradient descent would require hundreds of iterations, the normal equation solves everything in one computation.</p>

      <!-- AdSense - Middle of Article -->
      <div class="adsense-container">
        <!-- Replace with your AdSense code -->
        <p>AdSense Ad Placement - Middle of Article</p>
      </div>

      <h3>Step 4: Model Evaluation - Trust but Verify</h3>
      
      <p>Building a model is only half the battle. We must rigorously validate its performance:</p>

<pre><code class="language-python"># Calculate R-squared - how much variance we explain
ss_total = np.sum((y_true - np.mean(y_true)) ** 2)
ss_residual = np.sum((y_true - y_pred) ** 2)
r2 = 1 - (ss_residual / ss_total)

# Calculate RMSE - average prediction error
rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))</code></pre>

      <img src="../images/Normal_equation_ML.png" alt="Model validation results showing R² Score and RMSE performance metrics" style="max-width: 600px; margin: 20px auto; display: block;">

      <p>Our model achieves impressive results:</p>
      <ul>
        <li><strong>R² Score:</strong> 0.988 (explaining 98.8% of variance)</li>
        <li><strong>RMSE:</strong> 2.14 points (on a 0-100 scale)</li>
        <li><strong>No Overfitting:</strong> Test performance matches training performance</li>
      </ul>

      <h2 id="validating-your-results">Validating Your Results</h2>

      <h3>Understanding Residual Plots</h3>
      
      <p>Residual plots are like health checkups for your model. Each plot tells a different story:</p>
      <ol>
        <li><strong>Residuals vs Fitted Values:</strong> Random scatter confirms no systematic bias</li>
        <li><strong>Q-Q Plot:</strong> Points following the line confirm normal distribution</li>
        <li><strong>Histogram:</strong> Bell curve shape validates our assumptions</li>
        <li><strong>Scale-Location:</strong> Horizontal band shows consistent accuracy</li>
      </ol>

      <h3>Running the Complete Model</h3>
      
      <p>Here is how everything comes together:</p>

<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# First lets import the data
# Note: You will need to change this file path to the location of the dataset on your computer.
data_file = "~/path_to_your_file"

# create a pandas data frame
data_df = pd.read_csv(data_file)

# Lets print the dataset to see how it is structured.
print(data_df.head())

#Remapping "Extracurricular Activities"
data_df['Extracurricular Activities'] = data_df['Extracurricular Activities'].map({'Yes':1, 'No':0})

#Lets separate features from target
features = data_df.iloc[:, :-1]
target   = data_df.iloc[:, -1]

#Lets give each column names
features_names = features.columns.to_list()
target_name    = data_df.columns[-1]

#Time to check correlation
corr_m = data_df.corr()

#lets print values
print(corr_m['Performance Index'])

#lets create a figure for a different visualization
plt.figure(figsize=(8, 6))

# Create heatmap
sns.heatmap(corr_m, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix of Student Performance Features', fontsize=16)
plt.tight_layout()
plt.show()

#lets create a function to preprocess the dataset by normalizing it and splitting into 80% train and 20% test.
def data_processing(data_df, features_columns, target_column, train_size=0.8, random_state=42):

    #make copy of dataframe with the desired features and target
    desired_data = data_df[features_columns + [target_column]].copy()

    #shuffle dataset
    shuffle_df = desired_data.sample(frac = 1, random_state=random_state).reset_index(drop=True)

    #train_set
    train_sz  = int(train_size * len(desired_data))
    train_set = shuffle_df[:train_sz]

    #test_set
    test_set = shuffle_df[train_sz:]

    X_train = train_set[features_columns]
    y_train = train_set[target_column]
    X_test  = test_set[features_columns]
    y_test  = test_set[target_column]

    #apply standardization formula to features
    train_mean  = X_train.mean()
    train_std   = X_train.std()
    X_train_std = (X_train - train_mean) / train_std
    X_test_std  = (X_test - train_mean) / train_std

    return X_train_std, y_train, X_test_std, y_test

def train_and_eval_model(X_train, y_train, X_test, y_test, plot_residuals=True, plot_fit=True):
    # Create a feature matrix with intercept column
    n_train = X_train.shape[0]
    n_test  = X_test.shape[0]
    X_train_intercept = np.column_stack([np.ones(n_train), X_train])
    X_test_intercept  = np.column_stack([np.ones(n_test), X_test])

    # Solve normal equations beta = (X^T * X)^(-1) * X^T * y
    # Help to find the optimal coefficients in one direct computation
    X_T = X_train_intercept.T
    try:
        beta = np.linalg.solve(X_T @ X_train_intercept, X_T @ y_train)
    except np.linalg.LinAlgError:
        beta = np.linalg.lstsq(X_train_intercept, y_train, rcond=None)[0]

    # Generate predictions
    y_train_pred = X_train_intercept @ beta
    y_test_pred = X_test_intercept @ beta

    # Calculate residuals (actual - predicted)
    train_residuals = y_train - y_train_pred
    test_residuals = y_test - y_test_pred

    #Create Linear regression plot
    if plot_fit:
        create_prediction_vs_actual_plot(y_train, y_train_pred, y_test, y_test_pred)

    # Create residual plots
    if plot_residuals:
        create_residual_plots(y_train, y_train_pred, train_residuals,
                              y_test, y_test_pred, test_residuals)

    # Calculate performance metrics
    def compute_metrics(y_true, y_pred, dataset_name):
        # Mean squared error formula
        mse = np.mean((y_true - y_pred) ** 2)
        # Root mean squared error formula
        rmse = np.sqrt(mse)

        # R-squared: proportion of variance explained (1.0 = perfect, 0.0 = no better than mean)
        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)
        ss_residual = np.sum((y_true - y_pred) ** 2)
        r2 = 1 - (ss_residual / ss_total) if ss_total > 0 else 0

        return {
            f'{dataset_name}_mse': mse,
            f'{dataset_name}_rmse': rmse,
            f'{dataset_name}_r2': r2,
            f'{dataset_name}_samples': len(y_true)
        }

    # Compute metrics for both training and test sets
    train_metrics = compute_metrics(y_train, y_train_pred, 'train')
    test_metrics = compute_metrics(y_test, y_test_pred, 'test')

    # Display the metrics
    display_metrics_pandas(train_metrics, test_metrics)

    return {'coefficients': beta, 'train_metrics': train_metrics, 'test_metrics': test_metrics, 'predictions': { 'train': y_train_pred, 'test': y_test_pred},
        'residuals': {'train': train_residuals, 'test': test_residuals}}

def display_metrics_pandas(train_metrics, test_metrics):

    metrics_data = {
        'Training': [
            train_metrics['train_mse'],
            train_metrics['train_rmse'],
            train_metrics['train_r2'],
            train_metrics['train_samples']
        ],
        'Test': [
            test_metrics['test_mse'],
            test_metrics['test_rmse'],
            test_metrics['test_r2'],
            test_metrics['test_samples']
        ]
    }

    df = pd.DataFrame(metrics_data,
                      index=['MSE', 'RMSE', 'R²', 'Samples'])
    for col in df.columns:
        df.loc[['MSE', 'RMSE', 'R²'], col] = df.loc[['MSE', 'RMSE', 'R²'], col].round(4)

    print("\n" + "="*40)
    print("MODEL PERFORMANCE METRICS")
    print("="*40)
    print(df.to_string())
    print("="*40 + "\n")

def create_residual_plots(y_train, y_train_pred, train_residuals, y_test, y_test_pred, test_residuals):

    # Create a figure with 4 subplots
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('Residual Analysis for Model Validation', fontsize=16)

    # 1. Residuals vs Fitted Values
    ax1 = axes[0, 0]
    ax1.scatter(y_train_pred, train_residuals, alpha=0.5, label='Training', s=10)
    ax1.scatter(y_test_pred, test_residuals, alpha=0.5, label='Test', s=10)
    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)
    ax1.set_xlabel('Predicted Performance Index')
    ax1.set_ylabel('Residuals (Actual - Predicted)')
    ax1.set_title('Residuals vs Fitted Values')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. Q-Q Plot for Normality Check
    ax2 = axes[0, 1]
    all_residuals = np.concatenate([train_residuals, test_residuals])
    stats.probplot(all_residuals, dist="norm", plot=ax2)
    ax2.set_title('Q-Q Plot')
    ax2.grid(True, alpha=0.3)

    # 3. Histogram of Residuals
    ax3 = axes[1, 0]
    ax3.hist(train_residuals, bins=30, alpha=0.5, label='Training', density=True)
    ax3.hist(test_residuals, bins=30, alpha=0.5, label='Test', density=True)

    #normal distribution curve
    residual_std = np.std(all_residuals)
    residual_mean = np.mean(all_residuals)
    x = np.linspace(residual_mean - 4*residual_std, residual_mean + 4*residual_std, 100)
    ax3.plot(x, stats.norm.pdf(x, residual_mean, residual_std), 'r-',
             linewidth=2, label='Normal Distribution')
    ax3.set_xlabel('Residuals')
    ax3.set_ylabel('Density')
    ax3.set_title('Distribution of Residuals')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. Scale-Location Plot
    ax4 = axes[1, 1]
    train_std_residuals = train_residuals / np.std(train_residuals)
    test_std_residuals = test_residuals / np.std(test_residuals)
    ax4.scatter(y_train_pred, np.sqrt(np.abs(train_std_residuals)),
                alpha=0.5, label='Training', s=10)
    ax4.scatter(y_test_pred, np.sqrt(np.abs(test_std_residuals)),
                alpha=0.5, label='Test', s=10)
    ax4.set_xlabel('Predicted Performance Index')
    ax4.set_ylabel('Standardized Residuals')
    ax4.set_title('Scale-Location Plot')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def create_prediction_vs_actual_plot(y_train, y_train_pred, y_test, y_test_pred):

    plt.figure(figsize=(8, 6))

    # Scatter plot for training and test data
    plt.scatter(y_train, y_train_pred, alpha=0.5, label='Training Data', s=15)
    plt.scatter(y_test, y_test_pred, alpha=0.7, label='Test Data', s=15)

    # Line of perfect prediction (y=x)
    min_val = min(min(y_train), min(y_test))
    max_val = max(max(y_train), max(y_test))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

    plt.title('Predicted vs. Actual Performance Index')
    plt.xlabel('Actual Performance Index')
    plt.ylabel('Predicted Performance Index')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

[X_train, y_train, X_test, y_test] = data_processing(data_df,features_columns=['Previous Scores', 'Hours Studied', 'Sleep Hours'], target_column='Performance Index')
results = train_and_eval_model(X_train, y_train, X_test, y_test,plot_residuals=True, plot_fit=True)</code></pre>

      <h2 id="key-takeaways">Key Takeaways</h2>

      <h3>What You Have Accomplished</h3>
      
      <p>By building this model from scratch, you have:</p>
      <ul>
        <li><strong>Mastered the Mathematics:</strong> You understand the normal equation and how it finds optimal solutions</li>
        <li><strong>Implemented Core Algorithms:</strong> Your code handles data preprocessing, model training, and evaluation</li>
        <li><strong>Validated Thoroughly:</strong> You know how to check if a model is truly working</li>
        <li><strong>Achieved Excellence:</strong> 98.8% accuracy proves the power of understanding fundamentals</li>
      </ul>

      <h3>Next Steps in Your Journey</h3>
      <ol>
        <li><strong>Experiment with Features:</strong> Try different feature combinations and observe how performance changes</li>
        <li><strong>Compare with Scikit-learn:</strong> Implement the same model using scikit-learn and verify you get identical results</li>
        <li><strong>Scale to New Problems:</strong> Apply these techniques to other regression problems</li>
        <li><strong>Learn Gradient Descent:</strong> Implement the iterative approach and understand when each method is preferred</li>
      </ol>

      <h3>Resources for Continued Learning</h3>
      <ul>
        <li><strong>Full Implementation:</strong> Access the complete code on Kaggle</li>
        <li><strong>Dataset:</strong> Student Performance Dataset</li>
        <li><strong>Further Reading:</strong> Linear Algebra for Machine Learning, Understanding Statistical Learning</li>
      </ul>

      <h2>Final Thoughts</h2>
      
      <p>Building machine learning models from scratch transforms you from a library user into a true practitioner. You now possess knowledge that many developers lack - the ability to understand, debug, and optimize at the deepest level.</p>
      
      <p>Remember, every expert started exactly where you are now. The difference is they kept building, kept learning, and never stopped questioning how things work.</p>
       
      <p><strong>Your next challenge awaits. Start building.</strong></p>
      
    </article>

    <!-- Post Tags -->
    <div class="post-tags">
      <h4>Tags:</h4>
      <a href="#" class="tag">machine-learning</a>
      <a href="#" class="tag">python</a>
      <a href="#" class="tag">linear-regression</a>
      <a href="#" class="tag">data-science</a>
      <a href="#" class="tag">tutorial</a>
      <a href="#" class="tag">from-scratch</a>
      <a href="#" class="tag">beginner-friendly</a>
    </div>

    <!-- Social Sharing -->
    <div class="social-share">
      <h4>Share this post:</h4>
      <div class="share-buttons">
        <a href="#" class="share-btn share-twitter" onclick="shareOnTwitter()">
          <i class="bi bi-twitter"></i> Twitter
        </a>
        <a href="#" class="share-btn share-linkedin" onclick="shareOnLinkedIn()">
          <i class="bi bi-linkedin"></i> LinkedIn
        </a>
        <a href="#" class="share-btn share-facebook" onclick="shareOnFacebook()">
          <i class="bi bi-facebook"></i> Facebook
        </a>
      </div>
    </div>


    <!-- AdSense - Bottom of Article -->
    <div class="adsense-container">
      <!-- Replace with your AdSense code -->
      <p>AdSense Ad Placement - Bottom of Article</p>
    </div>
  </main>

  <!-- Vendor JS Files -->
  <script src="../../assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="../../assets/vendor/aos/aos.js"></script>
  <script src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../../assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="../../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../../assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="../../assets/vendor/typed.js/typed.min.js"></script>
  <script src="../../assets/vendor/waypoints/noframework.waypoints.js"></script>

  <!-- Prism.js for Code Highlighting -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-cpp.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>

  <!-- Blog Post JavaScript -->
  <script>
    // Social sharing functions
    function shareOnTwitter() {
      const url = window.location.href;
      const text = document.querySelector('.post-title').textContent;
      window.open(`https://twitter.com/intent/tweet?text=${encodeURIComponent(text)}&url=${encodeURIComponent(url)}`, '_blank');
    }

    function shareOnLinkedIn() {
      const url = window.location.href;
      window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${encodeURIComponent(url)}`, '_blank');
    }

    function shareOnFacebook() {
      const url = window.location.href;
      window.open(`https://www.facebook.com/sharer/sharer.php?u=${encodeURIComponent(url)}`, '_blank');
    }

    // Initialize AOS
    AOS.init();
  </script>
</body>
</html>